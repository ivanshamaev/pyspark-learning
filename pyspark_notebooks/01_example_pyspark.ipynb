{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28d4f300-35c0-496f-a8f5-041aae956c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: LinkageError occurred while loading main class org.apache.spark.launcher.Main\n",
      "\tjava.lang.UnsupportedClassVersionError: org/apache/spark/launcher/Main has been compiled by a more recent version of the Java Runtime (class file version 61.0), this version of the Java Runtime only recognizes class file versions up to 55.0\n",
      "/home/ivan/project/pyspark-learning/venv/lib/python3.12/site-packages/pyspark/bin/spark-class: line 97: CMD: bad array subscript\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPySparkRuntimeError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m col, avg, count, when\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Запуск Spark-сессии\u001b[39;00m\n\u001b[32m      5\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBasic PySpark Example\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Примерный датасет сотрудников\u001b[39;00m\n\u001b[32m     10\u001b[39m data = [\n\u001b[32m     11\u001b[39m     (\u001b[32m1\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAlice\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHR\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m4000\u001b[39m),\n\u001b[32m     12\u001b[39m     (\u001b[32m2\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBob\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mIT\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m5000\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m     (\u001b[32m8\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHenry\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mIT\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m5000\u001b[39m)\n\u001b[32m     19\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/pyspark-learning/venv/lib/python3.12/site-packages/pyspark/sql/session.py:556\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    554\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    559\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/pyspark-learning/venv/lib/python3.12/site-packages/pyspark/core/context.py:523\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/pyspark-learning/venv/lib/python3.12/site-packages/pyspark/core/context.py:205\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    201\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    208\u001b[39m         master,\n\u001b[32m    209\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m         memory_profiler_cls,\n\u001b[32m    220\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/pyspark-learning/venv/lib/python3.12/site-packages/pyspark/core/context.py:444\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    443\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/pyspark-learning/venv/lib/python3.12/site-packages/pyspark/java_gateway.py:111\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m    108\u001b[39m     time.sleep(\u001b[32m0.1\u001b[39m)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[32m    112\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mJAVA_GATEWAY_EXITED\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    113\u001b[39m         messageParameters={},\n\u001b[32m    114\u001b[39m     )\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[32m    117\u001b[39m     gateway_port = read_int(info)\n",
      "\u001b[31mPySparkRuntimeError\u001b[39m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, count, when\n",
    "\n",
    "# Запуск Spark-сессии\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Basic PySpark Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Примерный датасет сотрудников\n",
    "data = [\n",
    "    (1, \"Alice\", \"HR\", 4000),\n",
    "    (2, \"Bob\", \"IT\", 5000),\n",
    "    (3, \"Charlie\", \"IT\", 5500),\n",
    "    (4, \"Diana\", \"Marketing\", 4300),\n",
    "    (5, \"Evan\", \"HR\", 4100),\n",
    "    (6, \"Fay\", \"IT\", 6200),\n",
    "    (7, \"Grace\", \"Marketing\", 3800),\n",
    "    (8, \"Henry\", \"IT\", 5000)\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"department\", \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Вывод исходного датафрейма\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caabdbac-7e72-47fe-a4fa-2aa67ca362ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter (узкая операция)\n",
    "df_filtered = df.filter(col(\"salary\") > 4500)\n",
    "\n",
    "# Select и withColumn\n",
    "df_with_bonus = df.withColumn(\"salary_with_bonus\", col(\"salary\") * 1.1).select(\"name\", \"salary_with_bonus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f8386d-c187-433e-b3c5-9e262f8eda6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupBy + Aggregation (shuffle)\n",
    "avg_salary_by_dept = df.groupBy(\"department\").agg(avg(\"salary\").alias(\"avg_salary\"))\n",
    "\n",
    "# Join (shuffle, если по разным partition)\n",
    "departments = [\n",
    "    (\"HR\", \"Москва\"),\n",
    "    (\"IT\", \"Санкт-Петербург\"),\n",
    "    (\"Marketing\", \"Казань\")\n",
    "]\n",
    "dept_df = spark.createDataFrame(departments, [\"department\", \"city\"])\n",
    "\n",
    "df_joined = df.join(dept_df, on=\"department\", how=\"inner\")\n",
    "\n",
    "# Distinct (shuffle)\n",
    "distinct_departments = df.select(\"department\").distinct()\n",
    "\n",
    "# Sort (shuffle)\n",
    "sorted_df = df.sort(col(\"salary\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2f0f1b-4deb-4210-81e6-dfc021799290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Примеры actions:\n",
    "df_filtered.show()\n",
    "df_with_bonus.show()\n",
    "avg_salary_by_dept.show()\n",
    "df_joined.show()\n",
    "distinct_departments.show()\n",
    "sorted_df.show()\n",
    "\n",
    "# Подсчёт количества сотрудников в каждом отделе\n",
    "df.groupBy(\"department\").agg(count(\"*\").alias(\"num_employees\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
